import asyncio
import re
import torch
from aiogram import Bot, Dispatcher, Router
from aiogram.filters import Command
from aiogram.types import Message
from datetime import datetime, timedelta
from sqlalchemy import func
from sentence_transformers import SentenceTransformer, util
from sqlalchemy.orm import Session
from datetime import datetime, timedelta
from src.database.models import SessionLocal, Article
from src.database.models import SessionLocal, Article
from src.config.settings import TELEGRAM_API_KEY

bot = Bot(token=TELEGRAM_API_KEY)
dp = Dispatcher()
router = Router()

dp.include_router(router)

async def start_bot():
    """–§—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ Telegram-–±–æ—Ç–∞."""
    print("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω...")
    await dp.start_polling(bot)

def get_article_count_by_period(days=None):
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥."""
    session = SessionLocal()
    query = session.query(func.count(Article.id))
    if days:
        since_date = datetime.now() - timedelta(days=days)
        query = query.filter(Article.published_date >= since_date)
    count = query.scalar()
    session.close()
    return count

def get_articles_by_period(days=None):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥."""
    session = SessionLocal()
    query = session.query(Article)
    if days:
        since_date = datetime.now() - timedelta(days=days)
        query = query.filter(Article.published_date >= since_date)
    articles = query.order_by(Article.published_date.desc()).all()
    session.close()
    return articles

def search_article_by_title(title):
    """–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç—å–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é."""
    session = SessionLocal()
    article = session.query(Article).filter(Article.title.ilike(f"%{title}%")).first()
    session.close()
    return article

@router.message(Command("start"))
async def start_command(message: Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start."""
    await message.answer(
        "–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–∞—É—á–Ω—ã–º–∏ —Å—Ç–∞—Ç—å—è–º–∏. –í–æ—Ç —á—Ç–æ —è –º–æ–≥—É:\n"
        "/stats ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—Ç–∞—Ç–µ–π\n"
        "/list <–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ> ‚Äî —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –∑–∞ –º–µ—Å—è—Ü –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É\n"
        "/search <–Ω–∞–∑–≤–∞–Ω–∏–µ> ‚Äî –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç—å–∏\n"
        "\n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, `/stats` –∏–ª–∏ `/search machine learning`"
    )

@router.message(Command("stats"))
async def stats_command(message: Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /stats."""
    today_count = get_article_count_by_period(days=1)
    week_count = get_article_count_by_period(days=7)
    month_count = get_article_count_by_period(days=30)
    year_count = get_article_count_by_period(days=365)

    await message.answer(
        f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—Ç–∞—Ç–µ–π:\n"
        f"–°–µ–≥–æ–¥–Ω—è: {today_count}\n"
        f"–ó–∞ –Ω–µ–¥–µ–ª—é: {week_count}\n"
        f"–ó–∞ –º–µ—Å—è—Ü: {month_count}\n"
        f"–ó–∞ –≥–æ–¥: {year_count}"
    )

def escape_markdown(text):
    """–≠–∫—Ä–∞–Ω–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è Markdown."""
    return re.sub(r"([_\*\[\]\(\)~`>#+\-=|{}\.!])", r"\\\1", text)

model = SentenceTransformer('all-MiniLM-L6-v2')

@router.message(Command("list"))
async def list_command(message: Message):
    """–ö–æ–º–∞–Ω–¥–∞ /list —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å—Å—ã–ª–æ–∫."""
    session: Session = SessionLocal()
    try:
        args = message.text.split(maxsplit=1)
        if len(args) < 2:
            await message.answer("‚ùå –£–∫–∞–∂–∏—Ç–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –¥–ª—è –ø–æ–∏—Å–∫–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n`/list machine learning`", parse_mode="Markdown")
            return

        keyword = args[1].strip()
        if not keyword:
            await message.answer("‚ùå –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º.")
            return

        articles = (
            session.query(Article)
            .filter(Article.published_date >= datetime.now() - timedelta(days=30))
            .limit(50)
            .all()
        )

        if not articles:
            await message.answer("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π.")
            return

        article_titles = [article.title for article in articles]
        query_embedding = model.encode(keyword, convert_to_tensor=True)
        title_embeddings = model.encode(article_titles, convert_to_tensor=True)

        similarities = util.cos_sim(query_embedding, title_embeddings)
        sorted_indices = torch.argsort(similarities[0], descending=True).tolist()

        response = f"üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É: `{escape_markdown(keyword)}`\n"
        found = False
        for idx in sorted_indices:
            article = articles[idx]
            similarity = similarities[0, idx].item()
            if similarity < 0.2: 
                continue

            found = True
            title = escape_markdown(article.title)
            original_url = article.pdf_url or "–û—Ä–∏–≥–∏–Ω–∞–ª –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
            summary_command = escape_markdown(f"/summary_{article.id}")

            response += (
                f"- {title} (–î–∞—Ç–∞: {article.published_date.date()})\n"
                f"  üîó [{escape_markdown('–û—Ä–∏–≥–∏–Ω–∞–ª —Å—Ç–∞—Ç—å–∏')}]({original_url}) | "
                f"{summary_command}\n"
            )

        if not found:
            response = f"‚ùå –ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: `{escape_markdown(keyword)}`"

        for chunk in split_message(response):
            await message.answer(chunk, parse_mode="Markdown", disable_web_page_preview=True)

    except Exception as e:
        await message.answer("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∫–æ–º–∞–Ω–¥—ã.")
        print(f"–û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–µ /list: {e}")
    finally:
        session.close()

def split_message(text, max_length=4096):
    """–†–∞–∑–¥–µ–ª—è–µ—Ç –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è Telegram."""
    lines = text.splitlines()
    chunks = []
    current_chunk = ""

    for line in lines:
        if len(current_chunk) + len(line) + 1 > max_length:
            chunks.append(current_chunk)
            current_chunk = line + "\n"
        else:
            current_chunk += line + "\n"

    if current_chunk:
        chunks.append(current_chunk)

    return chunks

@router.message(lambda message: message.text.startswith("/summary"))
async def show_summary(message: Message):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –ø–æ–∫–∞–∑–∞ –∫—Ä–∞—Ç–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è."""
    session = SessionLocal()
    try:
        article_id = message.text[len("/summary"):].lstrip("_")
        
        if not article_id.isdigit():
            await message.answer("‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∫–æ–º–∞–Ω–¥—ã.")
            return
        
        article_id = int(article_id)
        article = session.query(Article).filter(Article.id == article_id).first()

        if not article:
            await message.answer("‚ùå –°—Ç–∞—Ç—å—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        elif not article.summary:
            await message.answer("‚ùå –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.")
        else:
            await message.answer(f"üìÑ –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏:\n{article.summary}")
    except Exception as e:
        await message.answer("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞.")
        print(f"–û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–µ –∫–æ–º–∞–Ω–¥—ã /summary: {e}")
    finally:
        session.close()

@router.message(Command("search"))
async def search_command(message: Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /search."""
    query = message.text[len("/search "):].strip()
    if not query:
        await message.answer("üîç –£–∫–∞–∂–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ –ø–æ—Å–ª–µ –∫–æ–º–∞–Ω–¥—ã.")
        return

    article = search_article_by_title(query)
    if not article:
        await message.answer(f"–°—Ç–∞—Ç—å—è —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º '{query}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        return

    response = (
        f"üìÑ –ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ç—å—è:\n"
        f"–ù–∞–∑–≤–∞–Ω–∏–µ: {article.title}\n"
        f"–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {article.published_date.date()}\n"
        f"üîó [–°–∫–∞—á–∞—Ç—å PDF]({article.file_path})\n"
        f"üîó [–û—Ä–∏–≥–∏–Ω–∞–ª —Å—Ç–∞—Ç—å–∏]({article.pdf_url})\n"
        f"\n–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è: {article.summary if article.summary else '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}"
    )
    await message.answer(response, parse_mode="Markdown")

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ Telegram-–±–æ—Ç–∞."""
    print("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω...")
    await bot.delete_webhook(drop_pending_updates=True)
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())
