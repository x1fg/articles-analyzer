Статья "HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation" посвящена оценке возможностей крупных языковых моделей в области генерации кода, который может сам себя вызывать. Авторы представляют два новых набора данных — HumanEval Pro и MBPP Pro, которые расширяют существующие наборы данных для более сложной оценки моделей. Эти наборы данных включают задачи, требующие от модели не только генерировать код, но и обеспечивать его корректное выполнение без ошибок. Исследование фокусируется на проверке способности моделей понимать контекст задачи, генерировать синтаксически и семантически правильный код и успешно выполнять сгенерированные функции. Результаты экспериментов показывают, что, несмотря на прогресс в разработке языковых моделей, генерация самовыполняемого кода остается сложной задачей, требующей дальнейших улучшений.