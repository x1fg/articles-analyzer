Статья "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs" исследует проблему чрезмерного анализа простых задач языковыми моделями, такими как o1. Авторы отмечают, что эти модели иногда демонстрируют тенденцию к "перемудриванию", что приводит к неожиданным ошибкам при решении элементарных задач, например, при сложении чисел. Исследование фокусируется на том, как архитектура и обучение языковых моделей могут способствовать такому поведению. Авторы предлагают методы и подходы, которые могут помочь моделям избегать излишнего усложнения простых задач, улучшая их точность и надежность в таких ситуациях.