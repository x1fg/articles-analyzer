Статья рассматривает подход к распределенному машинному обучению, называемый Distributed Mixture-of-Agents (DMoA), который предназначен для повышения эффективности вывода на периферийных устройствах с использованием крупных языковых моделей (LLM). Основная идея DMoA заключается в разделении модели на несколько агентов, каждый из которых выполняет отдельные вычислительные задачи на различных устройствах. Это позволяет оптимизировать использование вычислительных ресурсов и уменьшить задержки в сетях с ограниченной пропускной способностью. Авторы демонстрируют, что их подход повышает производительность и снижает потребление энергии по сравнению с традиционными методами вывода на периферии. Статья также обсуждает возможные применения и перспективы использования DMoA в области интернета вещей и других сценариях, где важны быстрая обработка данных и потребление ресурсов.