Статья представляет обзор методов ускорения работы больших языковых моделей (LLM) с использованием управления кэшем ключ-значение (KV Cache). Авторы исследуют различные подходы к оптимизации производительности LLM, сосредотачиваясь на эффективном управлении кэшем, который используется для хранения промежуточных результатов вычислений. В обзоре рассматриваются как аппаратные, так и программные решения, призванные уменьшить задержки и повысить скорость обработки текстов, что особенно актуально при выполнении задач в режиме реального времени. Основное внимание уделяется техникам, которые позволяют сократить объемы памяти, необходимые для хранения кэша, и уменьшить количество операций чтения-записи. Статья также обсуждает потенциальные направления для будущих исследований и улучшений в области ускорения LLM через оптимизацию KV Cache.