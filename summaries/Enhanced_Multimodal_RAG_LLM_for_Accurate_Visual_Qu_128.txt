Статья посвящена улучшенной модели Multimodal RAG-LLM, разработанной для более точного ответа на визуальные вопросы. Авторы объединяют возможности Retrieval-Augmented Generation (RAG) и больших языковых моделей (LLM) для обработки как текстовой, так и визуальной информации. Модель интегрирует текстовые и визуальные элементы, чтобы более точно понимать контекст и отвечать на вопросы, связанные с изображениями. Основные достижения включают в себя повышение точности ответов и улучшение способности модели к интерпретации сложных визуальных сцен. Исследование демонстрирует, что использование мультимодальных данных позволяет добиться значительных улучшений по сравнению с предыдущими подходами в области визуального вопросно-ответного анализа.